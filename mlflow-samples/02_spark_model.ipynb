{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3889dd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4159863f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://dp-batchplatform-stage/ds/model-repository/20', experiment_id='20', lifecycle_stage='active', name='spark-test', tags={}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://model-repository.stg.dreamplug.net/\")\n",
    "mlflow.set_experiment(\"spark-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e940a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.spark\n",
    "import os\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "            .config(\"spark.jars.packages\", \"org.mlflow:mlflow-spark:1.11.0\")\n",
    "            .master(\"local[*]\")\n",
    "            .getOrCreate())\n",
    "df = spark.createDataFrame([\n",
    "        (4, \"spark i j k\"),\n",
    "        (5, \"l m n\"),\n",
    "        (6, \"spark hadoop spark\"),\n",
    "        (7, \"apache hadoop\")], [\"id\", \"text\"])\n",
    "import tempfile\n",
    "tempdir = tempfile.mkdtemp()\n",
    "df.write.csv(os.path.join(tempdir, \"my-data-path\"), header=True)\n",
    "# Enable Spark datasource autologging.\n",
    "mlflow.spark.autolog()\n",
    "loaded_df = spark.read.csv(os.path.join(tempdir, \"my-data-path\"),\n",
    "                header=True, inferSchema=True)\n",
    "# Call toPandas() to trigger a read of the Spark datasource. Datasource info\n",
    "# (path and format) is logged to the current active run, or the\n",
    "# next-created MLflow run if no run is currently active\n",
    "with mlflow.start_run() as active_run:\n",
    "    pandas_df = loaded_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ac54616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.spark\n",
    "import os\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "spark = (SparkSession.builder\n",
    "            .config(\"spark.jars.packages\", \"org.mlflow:mlflow-spark\")\n",
    "            .master(\"local[*]\")\n",
    "            .getOrCreate())\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0) ], [\"id\", \"text\", \"label\"])\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "model = pipeline.fit(training)\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.spark.log_model(model, \"spark-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b287d39d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
